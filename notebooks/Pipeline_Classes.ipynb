{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pipeline Classes",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OePIL8JZ0cOs"
      },
      "source": [
        "#Pipeline Classes - AI Democracy\n",
        "\n",
        "This notebook is to implement the classes responsible for the preprocessing pipeline, note that it assumes that you already have passed the text to the TypoParser functions in previous notebooks. All these objects are design to work only with the text (X) and label (y) columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYzcm-V5K_I7"
      },
      "source": [
        "# Importing main librarie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlHXhgzc7S21",
        "outputId": "228c3315-43a4-40bd-92dd-181db96b9953"
      },
      "source": [
        "!pip install -U spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 277kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
            "Collecting thinc<8.1.0,>=8.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Collecting catalogue<2.1.0,>=2.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/10/dbc1203a4b1367c7b02fddf08cb2981d9aa3e688d398f587cea0ab9e3bec/catalogue-2.0.4-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/fa/d43f31874e1f2a9633e4c025be310f2ce7a8350017579e9e837a62630a7e/pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 34.0MB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 49.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107107 sha256=c435e4e0cfb3ccb0606c354e6a95ea6dbf71a0b9ff8f1ddba9536fc851dee699\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: spacy-legacy, catalogue, pydantic, srsly, thinc, typer, smart-open, pathy, spacy\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: smart-open 5.0.0\n",
            "    Uninstalling smart-open-5.0.0:\n",
            "      Successfully uninstalled smart-open-5.0.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.4 pathy-0.5.2 pydantic-1.7.4 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuWoEZPr7V1h",
        "outputId": "3bff99a7-192d-40ee-bd60-49d847013f31"
      },
      "source": [
        "!python -m spacy download pt_core_news_lg"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-10 15:29:05.448256: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting pt-core-news-lg==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.0.0/pt_core_news_lg-3.0.0-py3-none-any.whl (578.1MB)\n",
            "\u001b[K     |████████████████████████████████| 578.1MB 27kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from pt-core-news-lg==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (57.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (20.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.0.1)\n",
            "Installing collected packages: pt-core-news-lg\n",
            "Successfully installed pt-core-news-lg-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnE5ZNrmK-sp"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pickle\n",
        "import spacy\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models import Word2Vec, KeyedVectors"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnYH7GNdLFI1"
      },
      "source": [
        "# Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s344co0LEiD"
      },
      "source": [
        "with open('no_typos.pkl', 'rb') as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "X = df['Text']\n",
        "\n",
        "#I don't have the classification yet, so\n",
        "#y = df['classification'] #Binary 0 - interrupt; 1 - continuity"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUCVVx9MNRJD"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SPcd-e3NTnk"
      },
      "source": [
        "class CustomTokenizer():\n",
        "    \n",
        "    def __init__(self, mappers='', custom_specials='default'):\n",
        "        if custom_specials == 'default':\n",
        "            self.custom_specials = \"!\\\"#$%&'()*+,¸./:;<=>?@[\\]^_`{|}-–⎯—«»´°‘’…~ªº€0123456789\"\n",
        "        else:\n",
        "            self.custom_specials = custom_specials\n",
        "        \n",
        "        if mappers:\n",
        "            with open(mappers, 'rb') as f:\n",
        "                mappers = pickle.load(f)\n",
        "                self.person_map = mappers[0]\n",
        "                self.party_map = mappers[1]\n",
        "    \n",
        "    #Find the Names and Parties of politicians and make them as a unique token\n",
        "    #Ex.: \"Inês de Sousa Real\" --> [\"Inês de Sousa Real\"], not [\"Inês\", \"de\", \"Sousa\", \"Real\"]\n",
        "    def fit(self, dataframe):\n",
        "        person_mapper = {}\n",
        "        for person in pd.Series(dataframe['Person'].unique()).to_list():\n",
        "            person_mapper[''.join(person.lower().split())] = person\n",
        "        \n",
        "        party_mapper = {}\n",
        "        for party in pd.Series(dataframe['Party'].unique()).to_list():\n",
        "            party = str(party)\n",
        "            party_mapper[''.join(party.lower().split())] = party\n",
        "        \n",
        "        mappers = (person_mapper, party_mapper)\n",
        "\n",
        "        self.person_map = person_mapper\n",
        "        self.party_map = party_mapper\n",
        "\n",
        "        with open('mappers.pkl', 'wb') as f:\n",
        "            pickle.dump(mappers, f)\n",
        "\n",
        "    def remove_specials_chars(self, text):\n",
        "        for special_char in self.custom_specials:\n",
        "            text = text.replace(special_char, ' ')\n",
        "        text = text.replace('CDS PP', 'CDS-PP')\n",
        "        return text\n",
        "    \n",
        "    #Apply the mapper, so a name becomes a single concatenated lowered string\n",
        "    #ex.: \"Inês de Sousa Real\" --> \"inesdesousareal\"\n",
        "    def apply_mappers(self, text):\n",
        "\n",
        "        for person in self.person_map:\n",
        "             text = text.replace(self.person_map[person], person)\n",
        "\n",
        "        for party in self.party_map:\n",
        "            party = str(party)\n",
        "            text = text.replace(self.party_map[party], party)\n",
        "        \n",
        "        return text\n",
        "\n",
        "    def convert_text(self, text):\n",
        "        #converts to lowercase and split the words\n",
        "        text = text.lower()\n",
        "        words = text.split()\n",
        "        \n",
        "        return words\n",
        "    \n",
        "    def transform(self, X):\n",
        "        X = X.apply(self.remove_specials_chars)\n",
        "        X = X.apply(self.apply_mappers)\n",
        "        X = X.apply(self.convert_text)\n",
        "        return X\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRX0XTsiNQZG"
      },
      "source": [
        "tokenizer = CustomTokenizer()\n",
        "tokenizer.fit(df)\n",
        "X = tokenizer.transform(X)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOEHSrwh4PxD",
        "outputId": "a383fa2b-0ac1-422b-a8ee-2d4f56a6c474"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dirijo',\n",
              " 'um',\n",
              " 'abraço',\n",
              " 'a',\n",
              " 'todos',\n",
              " 'neste',\n",
              " 'regresso',\n",
              " 'dos',\n",
              " 'plenários',\n",
              " 'à',\n",
              " 'casa',\n",
              " 'da',\n",
              " 'democracia',\n",
              " 'esperávamos',\n",
              " 'que',\n",
              " 'nesta',\n",
              " 'altura',\n",
              " 'já',\n",
              " 'pudéssemos',\n",
              " 'ter',\n",
              " 'regras',\n",
              " 'mais',\n",
              " 'flexíveis',\n",
              " 'mas',\n",
              " 'infelizmente',\n",
              " 'os',\n",
              " 'números',\n",
              " 'e',\n",
              " 'as',\n",
              " 'consequências',\n",
              " 'concretas',\n",
              " 'não',\n",
              " 'nos',\n",
              " 'permitem',\n",
              " 'tal',\n",
              " 'e',\n",
              " 'portanto',\n",
              " 'continuamos',\n",
              " 'no',\n",
              " 'essencial',\n",
              " 'com',\n",
              " 'as',\n",
              " 'regras',\n",
              " 'que',\n",
              " 'presidiram',\n",
              " 'aos',\n",
              " 'últimos',\n",
              " 'plenários',\n",
              " 'da',\n",
              " 'sessão',\n",
              " 'legislativa',\n",
              " 'srs',\n",
              " 'deputados',\n",
              " 'da',\n",
              " 'nossa',\n",
              " 'ordem',\n",
              " 'do',\n",
              " 'dia',\n",
              " 'constam',\n",
              " 'declarações',\n",
              " 'políticas',\n",
              " 'porém',\n",
              " 'antes',\n",
              " 'disso',\n",
              " 'a',\n",
              " 'sr',\n",
              " 'secretária',\n",
              " 'mariadaluzrosinha',\n",
              " 'fará',\n",
              " 'o',\n",
              " 'favor',\n",
              " 'de',\n",
              " 'anunciar',\n",
              " 'a',\n",
              " 'entrada',\n",
              " 'de',\n",
              " 'algumas',\n",
              " 'iniciativas',\n",
              " 'tem',\n",
              " 'a',\n",
              " 'palavra',\n",
              " 'sr',\n",
              " 'secretária']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2Y65MQo0Wyx"
      },
      "source": [
        "# Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a5K2qJfp2tk"
      },
      "source": [
        "class StopwordsParser():\n",
        "\n",
        "    def __init__(self, stopwords_file=''):\n",
        "        self.stopwords = open(stopwords_file, 'r').read().splitlines()\n",
        "        \n",
        "    def fit(self):\n",
        "        pass\n",
        "\n",
        "    def remove_stopwords(self, text):\n",
        "        text = [token for token in text if token not in self.stopwords]\n",
        "        return text\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.apply(self.remove_stopwords)\n",
        "        return X\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnFnoPgAkiVJ"
      },
      "source": [
        "stopwords_parser = StopwordsParser('complete_stopwords_set.txt')\n",
        "X = stopwords_parser.transform(X)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-SUirNB1qze",
        "outputId": "ca0ca3c0-e435-4c6b-8ea7-5f05de7b2d83"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dirijo',\n",
              " 'abraço',\n",
              " 'todos',\n",
              " 'neste',\n",
              " 'regresso',\n",
              " 'plenários',\n",
              " 'casa',\n",
              " 'democracia',\n",
              " 'esperávamos',\n",
              " 'nesta',\n",
              " 'altura',\n",
              " 'pudéssemos',\n",
              " 'ter',\n",
              " 'regras',\n",
              " 'flexíveis',\n",
              " 'infelizmente',\n",
              " 'números',\n",
              " 'consequências',\n",
              " 'concretas',\n",
              " 'permitem',\n",
              " 'tal',\n",
              " 'portanto',\n",
              " 'continuamos',\n",
              " 'essencial',\n",
              " 'regras',\n",
              " 'últimos',\n",
              " 'plenários',\n",
              " 'sessão',\n",
              " 'legislativa',\n",
              " 'ordem',\n",
              " 'dia',\n",
              " 'constam',\n",
              " 'declarações',\n",
              " 'políticas',\n",
              " 'porém',\n",
              " 'antes',\n",
              " 'disso',\n",
              " 'secretária',\n",
              " 'mariadaluzrosinha',\n",
              " 'fará',\n",
              " 'favor',\n",
              " 'anunciar',\n",
              " 'entrada',\n",
              " 'algumas',\n",
              " 'iniciativas',\n",
              " 'palavra',\n",
              " 'secretária']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJlc_J5_4eWY"
      },
      "source": [
        "# Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F_Fg7o74f3c"
      },
      "source": [
        "class CustomLemmatizer():\n",
        "    def __init__(self, mappers=''):\n",
        "        self.nlp = spacy.load('pt_core_news_lg',\n",
        "                              exclude=['attribute_ruler', 'tok2vec', 'morphologizer',\n",
        "                                       'parser', 'senter', 'ner', 'attribute_ruler'])\n",
        "        self.nlp.max_length = 6136000\n",
        "\n",
        "        if mappers:\n",
        "            with open(mappers, 'rb') as f:\n",
        "                mappers = pickle.load(f)\n",
        "                self.person_map = mappers[0]\n",
        "                self.party_map = mappers[1]\n",
        "\n",
        "    def fit(self):\n",
        "        pass\n",
        "    \n",
        "    def undo_mapping(self, tokens):\n",
        "        #Deixando nomes de pessoas como tokens legiveis novamente\n",
        "        for i, word in enumerate(tokens):\n",
        "            if word in self.person_map:\n",
        "                tokens[i] = self.person_map[word]\n",
        "            elif word in self.party_map:\n",
        "                tokens[i] = self.party_map[word]\n",
        "        return tokens\n",
        "    \n",
        "    def normalize_tokens(self, tokens):\n",
        "        meaningful_string = ' '.join(tokens)\n",
        "        spacy_object = self.nlp(meaningful_string)\n",
        "        normalized_tokens = [token.lemma_ for token in spacy_object]\n",
        "        return normalized_tokens\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.apply(self.normalize_tokens)\n",
        "        X = X.apply(self.undo_mapping)\n",
        "        return X"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUkXGpAT4c-a"
      },
      "source": [
        "lemmatizer = CustomLemmatizer('mappers.pkl')\n",
        "X = lemmatizer.transform(X)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2-UpxG6GQDN",
        "outputId": "d93bef94-e8a5-4720-9e89-beba1e2352ff"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dirigir',\n",
              " 'abraçar',\n",
              " 'todo',\n",
              " 'neste',\n",
              " 'regressar',\n",
              " 'plenário',\n",
              " 'casar',\n",
              " 'democracia',\n",
              " 'esperar',\n",
              " 'nesta',\n",
              " 'altura',\n",
              " 'poder',\n",
              " 'ter',\n",
              " 'regrar',\n",
              " 'flexível',\n",
              " 'infelizmente',\n",
              " 'número',\n",
              " 'consequência',\n",
              " 'concreto',\n",
              " 'permitir',\n",
              " 'tal',\n",
              " 'portanto',\n",
              " 'continuar',\n",
              " 'essencial',\n",
              " 'regrar',\n",
              " 'último',\n",
              " 'plenário',\n",
              " 'sessão',\n",
              " 'legislativo',\n",
              " 'ordem',\n",
              " 'dia',\n",
              " 'constar',\n",
              " 'declaração',\n",
              " 'político',\n",
              " 'porém',\n",
              " 'antar',\n",
              " 'disso',\n",
              " 'Secretário',\n",
              " 'Maria da Luz Rosinha',\n",
              " 'fazer',\n",
              " 'favor',\n",
              " 'anunciar',\n",
              " 'entrar',\n",
              " 'algum',\n",
              " 'iniciativo',\n",
              " 'palavra',\n",
              " 'Secretário']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utKcfhGLkGh_"
      },
      "source": [
        "# Embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WNfSr6gkBGE"
      },
      "source": [
        "class CustomEmbeddings():\n",
        "    def __init__(self, model='', vector_size=20, window_size=2):\n",
        "        if model:\n",
        "            self.model = KeyedVectors.load_word2vec_format(model)\n",
        "        self.vector_size = vector_size\n",
        "        self.window_size = window_size\n",
        "\n",
        "    #If no model was given, then apply doc2vec as default\n",
        "    def fit(self, X):\n",
        "        documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(X)]\n",
        "        self.model = Doc2Vec(\n",
        "            documents=documents,\n",
        "            vector_size=self.vector_size,\n",
        "            window=self.window_size,\n",
        "        )\n",
        "\n",
        "    def save_model(self, document_name):\n",
        "        self.model.save(document_name)\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.apply(self.model.infer_vector)\n",
        "        return X"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzlcVmcK6o2z"
      },
      "source": [
        "embedding = CustomEmbeddings()\n",
        "embedding.fit(X)\n",
        "X = embedding.transform(X)\n",
        "embedding.save_model('embedding_model.txt')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzy78HEU6vIV",
        "outputId": "80dc71f2-3b21-4631-f84c-9d0cd903461b"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.07701786, -0.03248294, -0.19406416, -0.14431924, -0.10805328,\n",
              "       -0.02573857,  0.07646266, -0.09554859, -0.05867622,  0.07883334,\n",
              "       -0.19267483,  0.03703659, -0.11316131, -0.31990945, -0.05868788,\n",
              "        0.09510308, -0.04018577,  0.14918438,  0.34055197, -0.20006648],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}