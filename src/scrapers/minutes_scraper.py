# -*- coding: utf-8 -*-
"""Ata Scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10A_30YmAZHUWGScYYogbimo0uZ5pOR21
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=False)

import pandas as pd
import requests as re
from bs4 import BeautifulSoup

page1 = re.get('https://debates.parlamento.pt/catalogo/r3/dar/01/14/01')
page2 = re.get('https://debates.parlamento.pt/catalogo/r3/dar/01/14/02')

soup1 : BeautifulSoup = BeautifulSoup(page1.content,"html.parser")
soup2 : BeautifulSoup = BeautifulSoup(page2.content,"html.parser")

import re
# all_minutes = soup1.find_all(href=re.compile("catalogo"))
all_minutes = soup1.find_all('table',id=False)
links_list = []
table_1 =  all_minutes[0].tbody
table_2 = all_minutes[1].tbody
  # print(minute.tbody.td.a)
# print(links_list[0])
a_table_1 = table_1.find_all('a')
for a in a_table_1:
  links_list.append(a['href'])

a_table_2 = table_2.find_all('a')
for a in a_table_2:
  links_list.append(a['href'])
links_list

class Minute:
  def __init__(minute, title, date, path):
    minute.title = title
    minute.date = date
    minute.path = path
  def __str__(minute):
    return "Minute:\n  Title: %s\n  Date: %s\n  Path: %s" % (minute.title, minute.date, minute.path)
  def to_dict(minute):
    return {
        'Title': minute.title,
        'Date': minute.date,
        'Path': minute.path
    }

minutes_list = []

for link in links_list:
  minutes_list.append(Minute(f"DAR-{link.split('/')[7]}", link.split('/')[8], link))


df = pd.DataFrame.from_records(reversed([minute.to_dict() for minute in minutes_list]))

df.head(100)

!pip install selenium
!apt-get update # to update ubuntu to correctly run apt install
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

# Commented out IPython magic to ensure Python compatibility.
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException
from bs4 import BeautifulSoup

# %cd "/content"

def download_transcript_as_txt(path):
  wd = webdriver.Chrome('chromedriver',options=chrome_options)
  wd.get(f"https://debates.parlamento.pt{path}")

  js_download_button = WebDriverWait(wd, 10).until(EC.element_to_be_clickable((By.XPATH, "//a[contains(@data-reveal-id,'modalExport')]")))
  js_download_button.click()

  js_set_export_type_txt = WebDriverWait(wd, 10).until(EC.element_to_be_clickable((By.XPATH, "//input[@id='rTxt']")))
  js_set_export_type_txt.click()

  js_set_complete_doc_export = WebDriverWait(wd, 10).until(EC.element_to_be_clickable((By.XPATH, "//input[@id='rDocumentoCompleto']")))
  js_set_complete_doc_export.click()

  js_export_button = WebDriverWait(wd, 10).until(EC.element_to_be_clickable((By.XPATH, "//input[@id='exportar']")))
  js_export_button.click()
  # print(wd.page_source)
  print(f"Path {path} downloaded.")

# Commented out IPython magic to ensure Python compatibility.
def download_minutes(minutes_list: [Minute]):
  for minute in minutes_list:
    try:
      download_transcript_as_txt(minute.path)
    except NoSuchElementException:
      print(f"Minute not downloaded: {NoSuchElementException}")
# %mkdir "/content/drive/My Drive/AI Democracy /Transcripts/"
# %cd "/content/drive/My Drive/AI Democracy /Transcripts/"
download_minutes(minutes_list)

wd.close()